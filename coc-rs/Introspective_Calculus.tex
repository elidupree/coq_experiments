\documentclass{article}

\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage[nointegrals]{wasysym}
\usepackage{mathtools}
\usepackage{mathpartir}
\usepackage{nameref}
\usepackage{xcolor}

\usepackage[english]{babel}
\usepackage[autostyle]{csquotes}
\usepackage{amsmath}
\usepackage{wasysym}
\MakeOuterQuote{"}

%\usepackage[backend=biber,style=numeric]{biblatex}
%\addbibresource{Introspective_Calculus.bib}

\title{The Introspective Calculus}
\author{Eli Dupree}
\date{\today}

\DeclareMathSymbol{\mlq}{\mathord}{operators}{``}
\DeclareMathSymbol{\mrq}{\mathord}{operators}{`'}

\begin{document}
  \maketitle
  
  \section{Introduction}
  
  This document is a bare-bones explanation of my current definitions of this calculus.
  I plan to later develop it into a full, polished explanation, once I'm more confident in its soundness.
  
  In type theory, the source of paradoxes (like Russell's paradox and Girard's paradox) is this: First you construct a self-referential claim, then you prove that claim using (rules analogous to) the following axiom of propositional logic, which is unsound for self-referential claims:
  \begin{equation*}
    \tag{implication within hypotheticals, traditional}
    (C \rightarrow (A \rightarrow B)) \rightarrow ((C \rightarrow A) \rightarrow (C \rightarrow B))
    \vspace{0.8em}
  \end{equation*}
  
  Type theories typically avoid this problem by preventing the construction of self-referential claims. However, general-purpose computation is intrinsically capable of constructing self-referential claims. Thus, any such rules must forbid many programs that would otherwise be valid.
  
  \renewcommand{\implies}[1]{\xrightarrow{#1}}
  The present work's innovation is to \emph{allow} self-referential claims, but instead weaken the above axiom. We equip the "implies" relation with a level, where $A \implies{n} B$ means "You can reason from $A$ to $B$ by a proof with \emph{no more than} $n$ levels of hypotheticals." Now the axiom becomes:
  
  \newcommand{\lzero}{0}
  \newcommand{\lsucc}[1]{\mathcal{S} #1}
  \begin{equation*}
    \tag{implication within hypotheticals, resilient}
    (C \implies{\lsucc n} (A \implies{n} B)) \implies{\lzero} ((C \implies{\lsucc n} A) \implies{\lzero} (C \implies{\lsucc n} B))
    \vspace{0.8em}
  \end{equation*}
  
  Since the result is one level higher than $A \implies{n} B$, it cannot be used later to prove the same $A \implies{n} B$ claim that it's based on. This prevents a self-referential claim from being elevated to form a self-referential \emph{proof}.
  
  This protection allows us to remove the other limitations of type theories.
  In particular, the source of the name "Introspective Calculus" is that every rule of IC can be abstracted over within IC.



  
  Section \ref{fundamentals} (\textit{\nameref{fundamentals}}) gives a formal definition of IC. 
   
  \section{Uh}
  
  , which I call the \textbf{introspection principle}:
  
  
  \begin{center}
    For any concept we use in the metatheory, there should be formulas that represent it within the theory.
  \end{center}
  
  \subsection{Inference rules}
  
  IC is a \emph{proof system}; some formulas are \emph{true statements}, and we must describe \emph{inference rules} that can be used to derive one true statement from others. For example, this rule states that
  
  \begin{equation*}
    \tag{cut}
    ((A \to B) \wedge ((B \wedge C) \to D)) \to ((A \wedge C) \to D)\\
  \end{equation*}
  , like the familiar \emph{modus ponens}:
  
  \begin{equation*}
    \inferrule{A \to B\\\\A}{B}
  \end{equation*}
  
  The introspection principle asks that we define formulas to represent inference rules.
  In some sense, this is the \emph{only} thing we need to do – by the time we're done representing inference rules, the theory will be powerful enough to represent everything else.
  However, the concept of inference rules is fairly complicated, being made out of several sub-concepts:
  \begin{itemize}
    \item \textbf{Unordered sets} of premises, which are other formulas.
    \item \textbf{Variables} (the A and B above), which express that you can inject arbitrary formulas into particular locations within the inference rule, and say that the rule exists for all such values.
    \item Finally, the fact that you can combine multiple inference rules to draw further conclusions.
  \end{itemize}
  
  As our way of defining how inference rules work \emph{inside} IC, we need to describe inference rules \emph{of} IC. This will look like a self-referential definition, where everything has to be defined before anything works. So we begin with intuitive definitions, then formalize them as we go.
    
  \subsection{Basics}
  
  $\top$ (read as "true") is the trivially true proposition.
  
  $A \wedge B$ (read as "A and B") is logical conjunction, with the usual meaning.

  \newcommand{\objto}{\hookrightarrow}
  $\forall x, B$ (read as "for all x, B"), where $B$ is a formula that may contain usages of the name $x$, is universal quantification. When we state a rule with unbound variables, they are implicitly quantified this way. (For example, a rule stated as $A \wedge B \vdash A$ is implicitly $\forall A, \forall B, A \wedge B \vdash A$.)
  
  Both $A \vdash B$ (read as "A proves B") and $A \objto B$ (read as "A implies B") are representations of inference rules or logical implication, saying that if A is true, then B is true. However, this needs more explanation.
  
  The usual propositional logic – \emph{unlike} IC – postulates \emph{modus ponens}, an inference rule which states:
  
  \begin{equation*}
    \tag{modus ponens}
    A \wedge (A \objto B) \vdash B\\
  \end{equation*}
  
  thus saying that if $(A \objto B)$ and $A$ are both \emph{true}, then $B$ is \emph{true}. IC only says that if $(A \objto B)$ is an \emph{inference rule of a system}, and $A$ is true in that system, then $B$ is true in that system. IC's basic rule of implication is the following, which keeps metatheory levels separate by having the \emph{same} number of levels of "$\objto$" in each premise and conclusion:
  
  \begin{equation*}
    \tag{chain rule}
    (A \objto B) \wedge (B \wedge C \objto D) \vdash (A \wedge C \objto D)\\
  \end{equation*}
  
  This can be read as modifying the rule $(B \wedge C \objto D)$, satisfying the need for $B$ by plugging the output of $(A \objto B)$ into it. It's a generalization of transitivity – by setting $C$ to $\top$, you get $(A \objto B) \wedge (B \objto D) \vdash (A \objto D)$. And if we set both $A$ and $C$ to $\top$, we get:
  
  \begin{equation*}
    (\top \objto A) \wedge (A \objto B) \vdash (\top \objto B)\\
  \end{equation*}
  
  which reveals the relation to \emph{modus ponens}: It's the same except with the trivial-looking "$\top \objto$" keeping everything at the same implication level.
  
  So, what's the relationship between $\vdash$ and $\objto$? They are the exact same concept, just at different metatheory levels. At first glance, $\vdash$ defines an inference rule of IC itself, while $\objto$ is an internal object that represents an inference rule. But it's more subtle than that: if $\vdash$-rules have $\objto$ on both sides, and $\objto$-rules are supposed to represent $\vdash$-rules, then $\objto$-rules must have some third relation (call it $\looparrowright$) on both sides. This continues infinitely, and the same rules must exist on every level.\footnote{Propositional logic avoids this by making all later relations the same as the second one, but accepting that they will have different rules than the first.}
  
  Ultimately, we want to write all the levels using the same symbol. We can't make that rigorous yet. For now, assume that each rule does exist on every level, and the symbols $\vdash$ and $\objto$ are \emph{variables} which can be filled in with any inference-level and its corresponding object-level.
  
  \subsection{Variables}
  
  I say \emph{variables}, but what we really need to define is \emph{functions}. Variables are just the convenient way of expressing them to the reader.\footnote{"But," you say, "doesn't be introspection principle ask that if you're discussing variables in the metatheory, you should define a representation of them within the theory?" Well, yes – but they don't need to be part of the \emph{core calculus}, which is the main job of \emph{this paper}. A full programming language, which I plan to build around this core calculus, would naturally be able to implement functions that convert text into token-trees and token-trees into formulas of IC, and others such inconveniences of interfacing between raw mathematical objects and the human reader.} A rule $A \wedge B \vdash A$ needs a function that takes two arguments, injects the first at the locations we labeled $A$, and injects the second at the locations we labeled $B$.
  
  We use adjacency as function application (the formula $AB$ is the function $A$ applied to the input $B$), and define two fundamental functions, or "combinators", that can be composed to construct any other function.\footnote{Readers may recognize this as the SKI combinator calculus, my "const" as the K combinator, and "fuse" as the S combinator. These have about the same meanings as the German words that K and S are initialisms for in Schönfinkel's original paper.}
  
  \newcommand{\id}{\operatorname{\mathrm{id}}}
  \newcommand{\const}{\operatorname{\mathrm{const}}}
  \newcommand{\fuse}{\operatorname{\mathrm{fuse}}}
  \begin{align*}
    \const A\,B &\equiv A\\
    \fuse A\,B\,C &\equiv (AC)(BC)\\
  \end{align*}
  
  $\equiv$ (read as "equals") is definitional equality.
  This is conflated with logical equivalence: $A \equiv B$ is just a notation for $(A \objto B) \wedge (B \objto A)$. Equal formulas can be substituted for each other at any position within a larger formula:
  \begin{align*}
    (A \equiv B) &\vdash (CA \equiv CB)\\
    (A \equiv B) &\vdash (AC \equiv BC)\\
  \end{align*}
  
  "But wait," you say, "those rules only allow substituting within \emph{function applications}. What about substituting within formulas like $A \objto B$?"
  
  In the raw grammar of IC, every compound formula is made of nested function applications. Infix operators like $A \objto B$ are just a notation for applying a function $(\objto)$ to inputs $A$ and $B$; we can write this explicitly as $((\objto) A B)$.
  
  
  
  % If $A \equiv B$, then they are logically equivalent and can be substituted in any sub-formula:
  
  % \begin{align*}
  %   (A \equiv B) &\objto (A \objto B)\\
  %   (A \equiv B) &\objto (CA \equiv CB)\\
  %   (A \equiv B) &\objto (AC \equiv BC)\\
  % \end{align*}
  
  \newcommand{\nameabst}[1]{#1 \Rightarrow}
  
  Of course, usually, we want to write functions using variables. For this, we define a notation $\nameabst{x} B$ (read as "x goes to B" or "lambda x, B"), where $B$ is a formula that may contain usages of the name $x$. This expands to a bunch of combinators where $(\nameabst{x} B) C$ is equal to $B$ with the usages of $x$ replaced with $C$. (How do you do the conversion? It's called "abstraction elimination" and it's a well-known technique of the SKI combinator calculus; TODO put a full explanation here.)
  
  To express $\forall$, we introduce a new connective, $\bigwedge$ (read "$\bigwedge A$" as "big-and of $A$"), which means the universal conjunction over all possible inputs to a function. $\forall x, B$ is just a notation for $\bigwedge (\nameabst{x} B)$. Specialization is expressed by the rule $\bigwedge A\,\vdash AB$.
  
  \subsection{All rules of deduction}

  \begin{align*}
    \tag{and}
    (A \wedge B) &\vdash A\\
    \tag{symmetry}
    A \wedge B &\equiv B \wedge A\\
    \tag{associativity}
    (A \wedge B) \wedge C &\equiv A \wedge (B \wedge C)\\
    \tag{embed and}
    ((A \objto B) \wedge (A \objto C)) &\vdash (A \objto B \wedge C)\\
    \tag{embed And}
    (\forall X, A \objto B X) &\vdash (A \objto \bigwedge B)\\
    \tag{chain rule}
    (A \objto B) \wedge (B \wedge C \objto D) &\vdash (A \wedge C \objto D)\\
    A &\vdash \bigwedge \const A\\
    \bigwedge A &\vdash \bigwedge \fuse A\,B\\
    \tag{specialization}
    \bigwedge A &\vdash AB\\
    \const A\,B &\equiv A\\
    \fuse A\,B\,C &\equiv (AC)(BC)\\
    (A \equiv B) &\vdash (CA \equiv CB)\\
    (A \equiv B) &\vdash (AC \equiv BC)\\
  \end{align*}
  \newcommand{\deduction}{\operatorname{\mathrm{Deduction}}}
  Now, assemble all of these rules into one giant conjunction, which takes the relations $\vdash$ and $\objto$ as parameters. We will call this $\deduction$; the full definition goes like
  \begin{align*}
    \deduction& :=\\
    \nameabst{(\vdash)}&\ \nameabst{(\objto)}\\
    &\forall A, \forall B, (A \wedge B) \vdash A\\
    \wedge\ &\forall A, \forall B, A \wedge B \equiv B \wedge A\\
    \wedge\ &\dots\\
  \end{align*}
  
  And now we can define the \emph{single} symbol $(\to)$, which represents both inference rules of IC and internal representations of them.
  \\
  
  \textbf{Definition.} IC has $(\deduction\ (\to)\ (\to))$ as inference rules.
  
  \textbf{Definition.} IC has $(\deduction\ (\to)\ (\to))$ as a true statement, i.e. IC has $(\top \to \deduction\ (\to)\ (\to))$ as an inference rule.
  \\
  
  "But wait," you say, "if we can just plug in $(\to)$ as both inputs, why did we need to keep them separate?"
  
  Back to that in a minute. First, let's explore what statements count as true within the system so far.
  
  \subsection{Is \emph{modus ponens} true?}
  
  One may view a formal system is describing a \emph{set of true statements}. It does this by stating inference rules, which are \emph{constraints} on the set of true statements. In particular, inference rules are "positive constraints" – "if these premises are true, this conclusion must be true" – and never say that a statement must \emph{not} be true. Rather than having negative constraints, we just say that we're only interested in the set of statements that are \emph{forced} to be true by the inference rules – also known as the \emph{minimal relation obeying the inference rules}, or \emph{postulating induction on the structure of proofs}. And we hope that our system is \emph{consistent}, meaning that it does not force \emph{all} statements to be true.
  
  IC has an additional desire: Each true statement should represent a correct constraint on the set of true statements. And ideally, all correct constraints would also be true statements. But unfortunately, some constraints are self-incompatible: They are only correct if they are not also true statements (or if all statements are true). So we have to be slightly stricter about what statements can be "true".
  
  Let's consider the statement "Each true statement should represent a correct constraint on the set of true statements." By the introspection principle, we should be able to represent this statement in IC. Indeed, we can already represent the statement – and it is \emph{modus ponens!}
  
  "But," you say, "how does \emph{modus ponens} mean that?"
  
  Suppose you have a true statement $A \to B$. If we view this as a derived inference of IC, it is $\top \to (A \to B)$. If we also know that \emph{modus ponens}, also known as $(A \wedge (A \to B) \to B)$, is an inference of IC, then we can apply the chain rule – satisfying \emph{modus ponens}'s need for the premise $(A \to B)$ – and get $(A \wedge \top \to B)$, which reduces to $A \to B$, now on the level of an inference rather than a true statement. Thus, \emph{modus ponens} can be used to turn any true statement into an inference. This reflects our intent for the language; we must assert that it is a correct constraint.
  
  But what happens if we make it a true statement?
  
  \subsubsection{Russell's Paradox}
  
  First, we'll need some preliminaries.
  
  We define $\bot$ (read as "false") to equal $(\bigwedge \id)$, the proposition that all propositions are true. Clearly, this cannot be true in a consistent system.
  
  We would like to construct a statement $P$ such that $P \equiv P \to \bot$. To do this, we use Russell's Paradox, the "set of all sets which do not contain themselves". By convention, we represent \emph{sets} as predicates, and say that a set $S$ "contains" any $x$ for which $S x$ is true. So the "set of all sets which do not contain themselves" is:
  
  \begin{align*}
    R := \nameabst{x} x x \to \bot
  \end{align*}
  
  Now, our $P$ is simply $RR$:
  
  \begin{align*}
    P &:= RR\\
      &\equiv (\nameabst{x} x x \to \bot)R\\
      &\equiv RR \to \bot\\
      &\equiv P \to \bot\\
  \end{align*}
  
  Armed with this paradoxical statement, we can specialize \emph{modus ponens} with $A := P$ and $B := \bot$. The following statement is true, even if we do not assume \emph{modus ponens}:
  
  \begin{align*}
    modus\ ponens &\to (P \wedge (P \to \bot) \to \bot)\\
  \end{align*}
  
  But wait – $P \to \bot$ is just $P$, and $P \wedge P$ is just $P$, so this reduces:
  
  \begin{align*}
    modus\ ponens &\to (P \wedge (P \to \bot) \to \bot)\\
    modus\ ponens &\to (P \wedge P \to \bot)\\
    modus\ ponens &\to (P \to \bot)\\
    modus\ ponens &\to P\\
  \end{align*}
  
  If \emph{modus ponens} is both an inference rule and a true statement, we can now specialize the inference rule with $A := modus\ ponens$ and $B := P$, and thus infer that $P$ is true; and with one more use of the inference rule, we can infer that $\bot$ is true, and the system is inconsistent.
  
  Thus, \emph{modus ponens} is precisely one of our "self-incompatible constraints" – a constraint that is correct, but must not be a true statement! I choose to interpret this as \emph{modus ponens} being an "improper" rule of deduction, because of how it mixes truths with inferences, unlike all the rules in $\deduction$. Much of our remaining rules will be devoted to getting as close as possible to having \emph{modus ponens} be true, by adding a series of limited, safer versions of it.
  
  
    
  %(A \wedge (A \to B) \vdash B)
  
  \subsection{Induction on proof structure}
  
  Consider again the statement "Statements are only true if they're forced to be true by the inference rules." As with \emph{modus ponens}, we regard it to be true from the outside perspective. How shall we express this?
  
  We express it like this:
  
  \begin{align*}
    \tag{induction on proofs}\label{inp}
    \forall (\vdash), \forall A,\forall B,  (\deduction\ (\vdash)\ (\to)) \wedge (A \to B) \to (A \vdash B)\\
    \forall (\vdash), (\deduction\ (\vdash)\ (\to)) \to (A \to B) \to (A \vdash B)\\
    \forall (\models), (\deduction\ (\models)\ (\to)) \wedge (A \vdash B) \looparrowright (A \models B)\\
    (A \vdash B) \looparrowright \forall (\models), (\deduction\ (\models)\ (\to)) \looparrowright (A \models B)\\
  \end{align*}
  
  This deserves some explanation. Three different levels of inference are involved here (marked with numbers for the reader's benefit):
  
  \begin{align*}
    \forall (\vdash), \forall A,\forall B,  (\deduction\ (\vdash)\ (\implies{2})) \wedge (A \implies{1} B) \implies{0} (A \vdash B)\\
  \end{align*}
  
  We've already said that $(\deduction\ (\implies{1})\ (\implies{2}))$ is true. The idea now is to say that $\implies{1}$ is the \emph{minimal relation} obeying that constraint. We do this by saying that every other relation $\vdash$ obeying the same constraint is "at least as big" as $\implies{1}$, by saying that if $(A \implies{1} B)$, then $(A \vdash B)$ as well. ($(A \vdash B)$ may be true for \emph{more} things, but it can't be true for less.)
  
  This definition can also be used to write proofs by induction on the structure of proofs: If you describe an explicit $\vdash$, then the statements in $(\deduction\ (\vdash)\ (\implies{2}))$ are your induction cases.
  
  As with \emph{modus ponens}, we assert that this is a correct constraint on the true statements of IC. Again, this raises a question: Can \eqref{inp} itself be a true statement of IC?
  
  The answer, again, is no. Because $\implies{0}$ is equated with the other levels, this statement itself is a formula of the form $A \to B$, and it is not proved by $(\deduction\ (\to)\ (\to))$.
  
  
  \subsection{Uhh}
  
  We've now seen several constraints that cannot be true statements.
  
  Consider the statement "\emph{modus ponens} is a correct constraint (even though it may not be a true statement)". By the introspection principle, we should have a way to represent this statement within IC, and perhaps even have it be true.
  
  \newcommand{\icset}{\operatorname{\mathcal{IC}}}
  Let's have a symbol, $\icset$, which represents all inference rules of IC. (This will be self-containing, and that's okay.) Although we cannot soundly have $(\icset \to A) \to A$ as a truth, we are free to postulate that $\icset$ has the same \emph{set of true statements} as IC:
  
  \begin{align*}
    (\icset \to (\top \to A)) \equiv A\\
  \end{align*}
  
  We can now make statements about what inferences are allowed by $\icset$:
  
  \begin{align*}
    \icset \to modus\ ponens\\
  \end{align*}

  
  We'll have to refer to $(\deduction\ (\to)\ (\to))$ a lot. Let's define the shorthand $D_0 := (\deduction\ (\to)\ (\to))$.
  
  At this point, the \emph{truths} of IC are $D_0$ (by definition) and anything that can be deduced from it according to the inference rules, which are also $D_0$.
  
  We can also consider the statements $A$ where $(D_0 \to A)$ is true. This doesn't intrinsically include everything that's true in IC; if we added another rule ($\top \to Q$) to IC, it wouldn't give us ($\top \to (D_0 \to Q)$), because $Q$ cannot be deduced from just $D_0$, and ($\top \to Q$) does not add ($\top \to Q$) as a \emph{truth}, only $Q$. However, this category definitely contains $D_0$ and anything that can be deduced from it.
  
  Now, how about the statements $B$ where $(D_0 \to (D_0 \to B))$ is true?
  
  Clearly there's an infinite progression here. Viewing it from the outside, we believe that all of these categories contain the same statements. But we can't yet prove it within IC.
  
  The statement $\forall A, (D_0 \to A) \to A$
  
  \begin{align*}
    A\\
    D_0 \to A\\
    D_0 \to (D_0 \to A)\\
    D_0 \wedge (\top \to D_0) \to A\\
    D_0 \wedge (\top \to D_0) \to (\top \to A)\\
  \end{align*}

  \iffalse
  The introspection principle asks us to be able to discuss \emph{functions}, not just notations for functions. The meaning of \emph{functions} is how they map inputs to outputs – their \emph{extensional} behavior.
  \begin{align*}
    \tag{extensionality}
    %(A \equiv B) \equiv \forall R, (\forall A,\forall B,) \to (A \equiv B)
    \forall R, (\varnothing \to R A B) \cup (\forall C,\forall D,(\forall X,R (C X) (D X)) \to R C D) \to (A \equiv B)\\
  \end{align*}

  \begin{align*}
    A pred1 &\to \bigwedge A \to AB\\
    A prop \wedge B prop &\to A \wedge B \to B \wedge A\\
    \tag{and}
    A prop \wedge B prop &\to (A \wedge B) \to A\\
    \tag{associativity}
    A prop \wedge B prop \wedge C prop &\to (A \wedge B) \wedge C \leftrightarrow A \wedge (B \wedge C)\\
    \tag{map and}
    A prop \wedge B prop \wedge C prop &\to ((A \to B) \wedge (A \to C)) \to (A \to (B \wedge C))\\
    \tag{map And}
    A pred1 &\to (\forall X, A \to B X) \to (A \to \bigwedge B)\\
    \tag{cut}
    ((A \to B) \cup ((B \wedge C) \to D)) &\to ((A \wedge C) \to (D))\\
    A &\to \bigwedge \const A\\
    \bigwedge A &\to \bigwedge \fuse A\,B\\
    \const A\,B &\leftrightarrow A
  \end{align*}
  \begin{align*}
    \bigwedge A &\to AB\\
    AB &\to \bigvee A\\
    A \wedge B &\to B \wedge A\\
    A \vee B &\to B \vee A\\
    \tag{and}
    (A \wedge B) &\to A\\
    \tag{or}
    A &\to (A \vee B)\\
    \tag{associativity}
    (A \wedge B) \wedge C &\leftrightarrow A \wedge (B \wedge C)\\
    \tag{associativity}
    (A \vee B) \vee C &\leftrightarrow A \vee (B \vee C)\\
    \tag{map and}
    ((A \to B) \wedge (A \to C)) &\to (A \to (B \wedge C))\\
    \tag{map And}
    (\forall X, A \to B X) &\to (A \to \bigwedge B)\\
    \tag{map or}
    ((A \vee B) \to C) &\to ((A \to C) \vee (B \to C))\\
    \tag{map Or}
    (\bigvee A \to B) &\to (\exists X, A X \to B)\\
    \tag{cut}
    ((A \to B \vee J) \cup ((B \wedge C) \to D)) &\to ((A \wedge C) \to (D \vee J))\\
    \bigvee \const A &\to A\\
    \bigvee \fuse A\, B &\to \bigvee A\\
    A &\to \bigwedge \const A\\
    \bigwedge A &\to \bigwedge \fuse A\,B\\
    \const A\,B &\leftrightarrow A
  \end{align*}
  \begin{align*}
    \const A\,B &\equiv A\\
    \fuse A\,B\,C &\equiv (AC)(BC)\\
    \tag{identity}
    A &\equiv A \cup \varnothing\\
    \tag{symmetry}
    A \cup B &\equiv B \cup A\\
    \tag{associativity}
    (A \cup B) \cup C &\equiv A \cup (B \cup C)\\
    \tag{specialization}
    \bigcup A &\to A B\\
    \tag{specialization}
    \bigcup \const A &\equiv A\\
    \tag{specialization}
    \bigcup A &\to \bigcup \fuse A B\\
    \tag{specialization}
    \bigcup A &\equiv \forall B, \bigcup \fuse A B\\
    \tag{uhh}
    (A \to B) &\equiv (A \to (A \cup B))\\
    \tag{uhh}
    (A \cup B) &\to A\\
    \tag{uhh}
    (A \equiv B) &\equiv ((A \to B) \cup (B \to A))\\
    \tag{uhh}
    (A \equiv B) &\to (A \to B)\\
    \tag{indistinguishability}
    (A \equiv B) &\to (CA \equiv CB)\\
    \tag{extensionality}
    (A \equiv B) &\to (AC \equiv BC)\\
    \tag{unioning}
    ((A \to B) \cup (A \to C)) &\to (A \to (B \cup C))\\
    \tag{univ. unioning}
    ((\forall X, A \to B X) &\to (A \to \bigcup B)\\
    \tag{chaining}
    ((A \to B) \cup ((B \cup C) \to D)) &\to ((A \cup C) \to D)\\
  \end{align*}
  \fi
  
  
  \section{Fundamentals}\label{fundamentals}

  \subsection{Syntax}
  \begin{align*}
     Atom :=&\ \mathrm{level\_zero} \mid \mathrm{level\_successor} \mid \mathrm{implies} \mid \mathrm{equals} \mid \mathrm{const} \mid \mathrm{fuse} \mid \mathrm{induction\_on\_proofs}\\
     F :=&\ Atom \mid (F F)
  \end{align*}

  \subsection{Notations}

  % give examples first????
  % pronunciations of the symbols
  % AB is application
  % abstractions can be viewed as forall, lambda, function, predicates
  % substution better notation?
  % implication not subscript?
  % clean up explanation of metavariable levels WRT named abstractions
%  \[ A\ B\ C \dots Y\ Z := ((\dots ((A B) C) \dots Y) Z) \]

  \newcommand{\ic}[1]{#1}
%  \newcommand{\abst}[2]{{#1} \langle \overline{\underline{ #2}}]}
  
  
  \newcommand{\equals}{\equiv}

  \begin{align*}
    \lzero &:= \mathrm{level\_zero}\\
    \lsucc{n} &:= (\mathrm{level\_successor}\ n)\\
    (A \implies{n} B) &:= (((\mathrm{implies}\ n) A) B),\ \operatorname{right-associative}\\
    (A \equals B) &:= ((\mathrm{equals}\ A) B)\\
    \id &:= \fuse \const \const\\
  \end{align*}

  Finally, we define the \emph{named form} of abstractions, $(\nameabst{A}B),\ \operatorname{right-associative}$, where $A$ is a variable-name, and $B$ may contain instances of that name. (Perhaps we should say \emph{metavariable-name} rather than \emph{variable-name}, as these names only exist for this notation, and are not part of the formal language of IC).
  
  This is defined as in the SKI combinator calculus.

  \subsection{Axiom Definitions}
%  \setlength{\jot}{1.4em}
  \begin{gather*}
    \tag{modus ponens}\label{mp}
    \inferrule{(A \implies{n} B)\\\\A}{B}\\
  \end{gather*}
%  All remaining axioms are defined as implications ($Premise \implies{\wfz} Premise \dots \implies{\wfz} Conclusion$), which allows them to be reasoned about internally. (The external form can be derived using \eqref{mp}.) However, we still write them as $\frac{Premise\dots}{Conclusion}$ for readability.
%  \\\\
%  \setlength{\jot}{0.4em}
  ????:
  \begin{align*}
    \tag{specialization}\label{specialization}
    A &\implies{\lzero} AB\\
    \tag{equality implies implication}
    (A \equals B) &\implies{\lzero} (A \implies{\lzero} B)\\
%    \tag{truth of foralls}
%    A &\implies{\wfz} (\abst{A})\\
%    \tag{extensionality}\label{extensionality}
%    (\nameabst{C} (AC \equals BC)) \implies{\lzero} (A \equals B)\\
  \end{align*}
%  Definitional equality:
%  \begin{align*}
%%    \tag{symmetry}
%%    \inferrule{A \equals B}{B \equals A}\\
%  \end{align*}
  Elementary function definitions:
%  \setlength{\jot}{0.1em}
  \begin{align*}
    \const A\,B &\equals A\\
    \fuse A\,B\,C &\equals (AC)(BC)\\
    (A \equals B) C &\equals (CA \equals CB)\\
  \end{align*}
  Universal reasoning:
  \begin{align*}
    A &\implies{\lzero} \const A\\
    A &\implies{\lzero} \fuse A\\
    \tag{implication within universals}
    (\nameabst{C} (AC \implies{n} BC)) &\implies{\lzero} (A \implies{n} B)
  \end{align*}
  Implication:
  \begin{align*}
    \tag{weakening}
    B \implies{\lzero} (A \implies{\lzero} B)\\
    \tag{level weakening}
    (A \implies{n} B) \implies{\lzero} (A \implies{\lsucc n} B)\\
    \tag{implication within hypotheticals}
    (C \implies{\lsucc n} (A \implies{n} B)) \implies{\lzero} ((C \implies{\lsucc n} A) \implies{\lzero} (C \implies{\lsucc n} B))\\
  \end{align*}
  More stuff:
  \begin{align*}
    \tag{reflexivity}\label{eq_refl}
    A &\equals A\\
  \end{align*}

  \subsection{Internal Axioms}

  We now define \emph{internal} representations of the above axioms.

  From the internal perspective of the calculus, the above axioms provide truth for any \emph{specific} values of the variables $A, B\dots$, but cannot prove abstractions over those variables.
  So, for each axiom, we also create an internal form, by representing the metavariables using abstractions, and the premises using implication (at level $\lzero$).
  For example, (modus ponens) is represented as:

  \begin{equation*}
    \mathrm{``modus\ ponens"} := \nameabst{A} \nameabst{B} \nameabst{n} (A \implies{n} B) \implies{\lzero} A \implies{\lzero} B
  \end{equation*}
  
  The above is merely a notation, saying that the right-hand side \emph{is} the statement of "modus ponens". Rather than postulate each of these generalized axioms individually, we make the following comprehensive rule:
  
  \setlength{\jot}{0em}
  \begin{align*}
    \tag{proof induction is true}\label{indt}
    \mathrm{induction\_on\_proofs}\\
    \tag{definition of proof induction}\label{indd}
    \mathrm{induction\_on\_proofs} \equals \nameabst{P} (P \equals (&\nameabst{R}\\
      &\nameabst{n}\\
      &R\mathrm{``modus\ ponens"} \implies{\lzero}\\
      &\dots \implies{\lzero}\\
      &R\mathrm{``specialization"} \implies{\lzero}\\
      &R\ \mathrm{induction\_on\_proofs} \implies{\lzero}\\
      &(\nameabst{A}\nameabst{B}RA\implies{n}R(AB)) \implies{\lzero}\\
      &(\nameabst{A}\nameabst{B}R(A\implies{\lzero}B)\implies{n}RA\implies{n}RB) \implies{\lsucc{n}}\\
      &RP))\\
  \end{align*}

  (We would like the definition of proof induction to be a notation like the other axioms, but that would make it a self-containing formula. IC can actually permit self containing formulas, but we understand that the reader may be skeptical of a self containing axiom! Thus, we choose to represent it as an atom with a definitional-equality.)
  
  Observe that this rule immediately proves any of the individual generalized axioms. For example, "modus ponens" is proved by specializing this rule with $P := \mathrm{``modus\ ponens"}$, because the premise $R\mathrm{``modus\ ponens"}$ proves the conclusion $RP$.
  
  But the more interesting feature of this rule is the other direction of implication: If $P$, then $P$ is reachable by induction from the listed axioms. This essentially says that our list of axioms is \emph{exhaustive} – there are no other axioms – and the rule makes that fact visible to internal logic.
  
With such a powerful rule, one should immediately worry if it proves a contradiction. After all, the generalized axioms are not strictly the same objects as the external axioms, and the axiom \eqref{indd} is not in the list at all. However, any statement you can prove using an external axiom is a specialization of the generalized form, and thus reachable by induction from the generalized axioms. And because definitionally-equal formulas are indistinguishable to the internal logic, \eqref{indd} is just a specialization of \eqref{eq_refl}.

  \section{Examples}\label{structure}

  We can define the conventional logical connectives in terms of the above rules:

  \setlength{\jot}{0.4em}
  \begin{align*}
    \tag{false/absurdity}
    \bot &:= (\nameabst{P}{P})\\
    \tag{negation}
    \neg_{n} P &:= ({P \implies{n} \bot})\\
    \mathrm{not} &:= (\nameabst{P} \neg P)\\
%    \tag{conjunction}
%    P \land Q &:= (\nameabst{R} (P \implies{\wfz} Q \implies{\wfz} R) \implies{\wfz} R)\\
%    \mathrm{and} &:= (\nameabst{P}\nameabst{Q} P \land Q)\\
%    \tag{disjunction}
%    P \lor Q &:= (\nameabst{R} (P \implies R) \implies (Q \implies R) \implies R)\\
%    \mathrm{or} &:= (\nameabst{P}\nameabst{Q} P \lor Q)\\
%    \tag{existential quantification}
%    \exists x, P(x) &:= (\nameabst{R} (\nameabst{x} P(x) \implies R) \implies R)\\
  \end{align*}

%  And we can prove some tautologies:
%  \begin{align*}
%  (P \land Q)
%  \end{align*}
\iffalse
  \subsection{Well-founded sets}

  Some discussion of well-founded sets, and how they avoid the Burali-Forti paradox.

  By convention, a \emph{set} is a predicate on formulas, where if $PF$ is true, then $F \in P$. One might expect this to cause paradoxes; this section will illustrate how it does not.

  We define \emph{well-founded sets} with a surprisingly simple rule: A well-founded set is a set for which all \emph{raisable predicates} are true. And a \emph{raisable predicate} $R$ is one where, if the $R$ is true for all members of a set, then $R$ is also true for the set itself:

  \ic{
  Raisable := R => (Q => (x => (Q x) -> (R x)) -> (R Q)).
  WellFounded := S => R => (Raisable R) -> (R S).
  }

  The first well-founded set is the empty set, which we can prove is well-founded:
  
%  \ic{
%  EmptySet := n => (x => x).
%
%  (WellFounded EmptySet) = (R => (Q => (m => (Q m) -> (R m)) -> (R Q)) -> R(n => (x => x))).
%
%  (Some axiom) proves ((R m) -> (R m))
%  (Generalization) proves (m => (R m) -> (R m))
%  (Specialization) proves (Q => (m => (Q m) -> (R m)) -> (R Q))
%                       -> (Q => (m => (Q m) -> (R m)) -> (R Q))EmptySet
%  = (m => (EmptySet m) -> (R m)) -> (R EmptySet)
%  = (m => (x => x) -> (R m)) -> (R EmptySet)
%  (Predicate unfolding) proves (Proven_a (Q => (m => (Q m) -> (R m)) -> (R Q))EmptySet) -> (m => (EmptySet m) -> (R m)) -> (R EmptySet)
%
%
%  }

  \section{Consistency}\label{consistency}
  
  We regard a proposition $P$ as "proven" if it is \emph{eventually true} within IC, i.e.
  
  \newcommand{\eventually}{\operatorname{\mathrm{Eventually}}}
  
  \[ \eventually P := \exists n, \clocksub{+n} P \]
  
  This is a meta-theoretical definition; if we use our internal definition of $\exists$, which contains 2 nested abstractions, then you can never prove $(\eventually P)$, only $\clocksub{+2}(\eventually P)$.
  
  That said, since every axiom of IC can be applied within clocks, we could view the inside of $\clocksub{+n}$ as being another instance of the calculus – call it IC$_{+n}$ – which adds the axiom "if $\clocksub{+n} P$ is true in IC, then $P$ is true in IC$_{+n}$". If we move to IC$_{+2}$, we \emph{can} be comfortable with an internal definition of $\eventually$. Thus, IC can function as its own metatheory, just with a clock offset.
  
  By the axiom \eqref{lne}, anything that's eventually true is also eternally true, in the sense that you can "wait" for an IC$_{+n}$ where P is as old as you want:
  
%  \[ \nameabst{n} \wellfounded n \implies \eventually P \implies \eventually \clocksub{-n} P \]
  
  One might ask: What if you can't prove $(\eventually P)$, but only $\eventually (\eventually P)$? One might hope that $\eventually (\eventually P)$ implies $(\eventually P)$, but it does not – it only implies $\clocksub{+constant}(\eventually P)$. But could we also regard that as "proving $P$"? Unfortunately, if we did, we would then want to extend it to $\eventually (\eventually (\eventually P))$, and so forth. For the sake of having a single definition, the answer must be \emph{no}. But the purpose of including the entire well-founded hierarchy is that you don't \emph{need} to.
  
  Intuitively, the only way to prove $(\eventually P)$ is to exhibit a \emph{specific} $n$ where you can prove $\clocksub{+n} P$. Assuming this is true, you can simply take that value of $n$ Unfortunately, you can't prove this within IC, because the internal logic of IC doesn't know that there aren't extra axioms that could make $\eventually P$ true for other reasons. However, what you \emph{can} do is to define a predicate
  
  \newcommand{\provable}{\operatorname{\mathrm{Provable}}}
  
  \[\provable P := \dots\]
  
  which is true for anything that can be proved from the actual axioms of IC. (You define this as an induction predicate, with the axioms of IC as cases; we omit the definition because it is long.)
  
  A digression on the properties of $\provable$: Meta-theoretically, we don't think you can prove anything that's not $\eventually \provable$. But this is impossible to prove inside IC, for the same reasons as above. In fact, the proposition $(P \implies \eventually (\provable P))$ is \emph{false} within IC, because it is self-contradictory. (If true, it would be a statement that was true but not provable from the axioms!)
  
%  We also cannot prove $(\provable P) \implies P$, even though we believe that all provable statements are true; but fortunately, $(\provable P) \implies (\eventually P)$ is a trivial corollary.
  
  (All of the following is conjecture, but I expect it to be true, at least once I have cleaned up any definitional mistakes:)
  
  With the provability predicate, we can prove:
  
%  \[ \nameabst{P} \eventually (\provable (\eventually P)) \implies \eventually  P \]
  
  This theorem has an immediate corollary that is very significant:
  
%  \[ \nameabst{P} \eventually (\provable (\eventually \bot)) \implies \eventually \bot \]
\fi

  %\printbibliography
\end{document}
