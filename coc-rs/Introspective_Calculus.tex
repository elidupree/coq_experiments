\documentclass{article}

\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage[nointegrals]{wasysym}
\usepackage{mathtools}
\usepackage{mathpartir}
\usepackage{nameref}
\usepackage{xcolor}

\usepackage[english]{babel}
\usepackage[autostyle]{csquotes}
\usepackage{amsmath}
\usepackage{wasysym}
\MakeOuterQuote{"}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

%\usepackage[backend=biber,style=numeric]{biblatex}
%\addbibresource{Introspective_Calculus.bib}

\title{The Introspective Calculus}
\author{Eli Dupree}
\date{\today}

\DeclareMathSymbol{\mlq}{\mathord}{operators}{``}
\DeclareMathSymbol{\mrq}{\mathord}{operators}{`'}

\begin{document}
  \maketitle
  
  \section{Introduction}
  
  %This document is a bare-bones explanation of my current definitions of this calculus.
  %I plan to later develop it into a full, polished explanation, once I'm more confident in its soundness.
  
  In type theory, the source of paradoxes (like Russell's paradox and Girard's paradox) is this: First you construct a self-referential claim, then you prove that claim using (rules analogous to) the following axiom of propositional logic, which is unsound for self-referential claims:
  \begin{equation}
    \label{unsoundaxiom}
    %\tag{implication within hypotheticals, traditional}
    (C \to (A \to B)) \to ((C \to A) \to (C \to B))
    \vspace{0.8em}
  \end{equation}
  
  Type theories typically avoid this problem by preventing the construction of self-referential claims.
  However, general-purpose computation is intrinsically capable of constructing self-referential claims.
  Thus, any such rules must forbid many programs that would otherwise be valid.

  The present work's innovation is to \emph{allow} self-referential claims, but instead weaken the above axiom.
  We regard the axiom as \emph{improperly mixing metatheory levels}, for reasons we will explain in Section \ref{firststeps}.
  By giving explicit, principled definitions of the relationships between metatheory levels, we can prevent a self-referential claim from being elevated into a self-referential \emph{proof}.
  We call our system the Introspective Calculus (IC), because every rule of IC can be represented as an object within IC; it comes as close as possible to acting as its own metatheory.
  
  \iffalse
  For the following reason:
  
  \begin{itemize}
    \item We say that $(A \to B)$ is true if there is a \emph{chain of valid inferences} that would let you reason from $A$ to $B$.
    \item Thus, $(A \to (A \to B))$ means that if $A$ is true, you can infer that there \emph{exists} a chain of valid inferences from $A$ to $B$ (but if $A$ is not true, there might not be one). \eqref{unsoundaxiom} asserts that this implies $(A \to B)$, i.e. that such a chain already exists. Intuitively, this might seem true: If you first assume $A$, you can conclude that the $(A \to B)$ chain exists, and then apply it to get $B$.
    \item But inferences are metatheoretic objects, and the above is metatheoretic reasoning. So $(A \to B)$ will be true in the metatheory, but this doesn't mean we can automatically internalize it as a provable statement within the theory. Indeed, there \emph{must} be statements that are true, but not provable within the theory; and if we allow self-referential claims, this must be one of them, or we suffer Russell's paradox.
  \end{itemize}

  
  
    \renewcommand{\implies}[1]{\xrightarrow{#1}}
  \newcommand{\lzero}{0}
  \newcommand{\lsucc}[1]{\mathcal{S} #1}
  \begin{equation*}
    \tag{implication within hypotheticals, resilient}
    (C \implies{\lsucc n} (A \implies{n} B)) \implies{\lzero} ((C \implies{\lsucc n} A) \implies{\lzero} (C \implies{\lsucc n} B))
    \vspace{0.8em}
  \end{equation*}
  
  Since the result is one level higher than $A \implies{n} B$, it cannot be used later to prove the same $A \implies{n} B$ claim that it's based on. This prevents a self-referential claim from being elevated to form a self-referential \emph{proof}.
  
  This protection allows us to remove the other limitations of type theories.
  In particular, the source of the name "Introspective Calculus" is that every rule of IC can be abstracted over within IC.



  
  Section \ref{fundamentals} (\textit{\nameref{fundamentals}}) gives a formal definition of IC.   \fi
   
  \section{First steps}\label{firststeps}
  
  [everything is guided by] [this principle], which I call the \textbf{introspection principle}:
  
  
  \begin{center}
    For any concept we use in the metatheory, there should be formulas that represent it within the theory.
  \end{center}
 
  IC is a system of \emph{formulas}; some formulas represent \emph{propositions}, which may be true or false;

  
  \subsection{[The paradox / What is truth?]}
  
  We want to be able to talk about whether propositions are true or not. The introspection principle says we should be able to write \emph{formulas} that talk about whether other formulas are true or not.
  
  If we can do this, then we can use Russell's paradox to construct a statement $P$, where:
  
  \begin{equation*}
    P \equiv (P \mathrm{\ is\ not\ }\mathpzc{true})\\
  \end{equation*}
  
  (This formula contains a copy of itself.
  It may not be obvious that this should be allowed; but we assert that it is.
  We will later show how self-containing formulas arise naturally from how we define computation.
  For now, simply assume that they are just as legitimate as other formulas.)
  
  What shall we say of this formula?
  When it says $\mathpzc{true}$, that word must mean something; if it means anything like what you'd expect, then $P$ can't possibly be $\mathpzc{true}$, because if it was, then it wouldn't be.
  That last sentence forms a proof that $P$ is not $\mathpzc{true}$; therefore "$P \mathrm{\ is\ not\ }\mathpzc{true}$" is true, which is the same as saying that $P$ is true.
  To avoid a contradiction, we must say that $P$ is "true, but not $\mathpzc{true}$" – that is, that "$\mathpzc{true}$" and "true" are not the same concept.
  
  ...but now the introspection principle says we should be able to write $(P \mathrm{\ is\ true,\ but\ not\ }\mathpzc{true})$ as a formula, and have it be true.
  Which, in turn, allows us to write
  
  \begin{equation*}
    Q \equiv (Q \mathrm{\ is\ neither\ }\mathpzc{true}\mathrm{\ nor\ true})\\
  \end{equation*}
  
  which would require a third concept of truth to describe it.
  In fact, presumably we would need to keep making new concepts of truth forever.
  
  That's exactly what we will do. We will end up with an ordinal hierarchy of concepts of truth, each weaker than all the earlier ones.
  
  "But wait," you say, "what if I then write:"
  
  \begin{equation*}
    R \equiv (R \text{ is not true according to any level of the ordinal hierarchy})\\
  \end{equation*}
  
  This is where we stop: It won't be possible to write this. This is unfortunately the one place where we can't honor the introspection principle to-the-letter. But [later] we will [hopefully] show a metatheorem which says we don't \emph{need} to – because, for any reason you might need to refer to the entire ordinal hierarchy, the same need can be satisfied by one of the levels of the ordinal hierarchy.

  For the rest of this paper, we reserve the word "true" to refer to the broadest category: things that are true from an outside perspective. This concept can't be named in a formula. To start the hierarchy of nameable concepts, we need to build some more definitions.

  \subsection{Truth predicates}
  
  [since we have multiple concepts of truth, we need to formalize them:]
  
  A \textbf{truth predicate} is a predicate on propositions – a function that takes propositions and classifies them as "true" or not. Formally, its inputs are propositions, and its outputs are also propositions. If you have a truth predicate $\mathbb{T}$ and a proposition $A$, then if the proposition $\mathbb{T} A$ is \emph{true}, we say that $\mathbb{T}$ believes $A$; if $\mathbb{T} A$ is \emph{not true}, we say that $\mathbb{T}$ does not believe $A$. (But it doesn't necessarily believe that $A$ is false.)
  
  So, what's a proposition? Intuitively, a proposition like $A \wedge B$ is saying that two other propositions, $A$ and $B$, are both "true". Here's the catch: We can't judge the truth of $A \wedge B$ until we know which truth predicate it means when it says "true" – but we also want to say that we're discussing the \emph{same proposition} across multiple truth predicates.
    
  Thus, we define a \textbf{proposition} as a \emph{predicate on truth predicates} – a function that takes truth predicate and classifies whether the proposition is "true" \emph{within} that truth predicate. Formally, its inputs are truth predicates, and its outputs are propositions\footnote{At this point, the reader may wonder: "If a proposition is just a function that returns more propositions, when does it end?" This is a misconception. Just like in the lambda calculus, everything is a function forever, and that's okay.}. If you have a proposition $A$ and a truth predicate $\mathbb{T}$, then if the proposition $A \mathbb{T}$ is \emph{true}, we say that $A$ \emph{describes} $\mathbb{T}$; if $A \mathbb{T}$ is \emph{not true}, we say that $A$ \emph{does not describe} $\mathbb{T}$.
  
  For example, if $\mathbb{T}$ believes only the propositions $A$, $B$, and $C$, then the proposition $(A \wedge B)$ \emph{describes} $\mathbb{T}$, but $\mathbb{T}$ does not \emph{believe} $(A \wedge B)$. We can write the same thing as a formula: $((A \wedge B)\mathbb{T} \wedge \neg \mathbb{T}(A \wedge B))$.
  
  Before we can start defining concrete propositions, we need to formalize how we define functions.
  
  \subsection{Formulas and functions}
  
  The raw grammar of IC is:
  
  \newcommand{\id}{\operatorname{\mathrm{id}}}
  \newcommand{\const}{\operatorname{\mathrm{const}}}
  \newcommand{\fuse}{\operatorname{\mathrm{fuse}}}
  \newcommand{\atomimplies}{\operatorname{\mathrm{implies}}}
  \newcommand{\all}{\bigwedge}
  \begin{align*}
    Atom &:= \const \mid \fuse \mid \atomimplies \mid \all\\
    F &:= Atom \mid FF \\
  \end{align*}
  
  We will often write formulas using fancier "notations", which are convenient shorthands which expand to raw formulas of IC ($F$ above).
  
  Adjacency means function application: The formula $AB$ is the function $A$ applied to the input $B$. We sometimes use parentheses to specify the order of operations; when we don't, it is left-associative ($ABC$ means $(AB)C$).
  
  We define two fundamental functions, or "combinators", that can be composed to construct any other function.\footnote{Readers may recognize this as the SKI combinator calculus. My "const" is the K combinator, and my "fuse" is the S combinator. These names have about the same meanings as the German words (K)onstanzfunktion and Ver(s)chmelzungsfunktion, which K and S are named for in Schönfinkel's original paper. Before I learned of SKI, I had to reinvent it for myself; but after learning of it, I renamed my "fuse" combinator to be more aligned with its naming scheme, because my original name, "apply", was no less confusing.}
  
  
  \begin{align*}
    \const A\,B &\equiv A\\
    \fuse A\,B\,C &\equiv (AC)(BC)\\
  \end{align*}
  
  $\equiv$ (read as "equals") is definitional equality. The notation $A \equiv B$ expands to the raw formula "$equals A\,B$". Equal formulas can be substituted for each other at any position within a larger formula:
  \begin{align*}
    (A \equiv B) &\to (CA \equiv CB)\\
    (A \equiv B) &\to (AC \equiv BC)\\
  \end{align*}  
  
  % If $A \equiv B$, then they are logically equivalent and can be substituted in any sub-formula:
  
  % \begin{align*}
  %   (A \equiv B) &\objto (A \objto B)\\
  %   (A \equiv B) &\objto (CA \equiv CB)\\
  %   (A \equiv B) &\objto (AC \equiv BC)\\
  % \end{align*}
  
  \newcommand{\nameabst}[1]{#1 \Rightarrow}
  
  Of course, usually, we want to write functions using variables. For this, we define a notation $\nameabst{x} B$ (read as "x goes to B" or "lambda x, B"), where $B$ is a formula that may contain usages of the name $x$. This expands to a bunch of combinators where $(\nameabst{x} B) C$ is equal to "$B$ except the usages of $x$ are replaced with $C$". (How do you do the conversion? It's called "abstraction elimination" and it's a well-known technique of the SKI combinator calculus; TODO put a full explanation here.)
  
  To express $\forall$, we introduce a new connective, $\bigwedge$ (read "$\bigwedge A$" as "all of $A$" or "big-and of $A$"), which means the universal conjunction over all possible inputs to a function. $\forall x, B$ is just a notation for $\bigwedge (\nameabst{x} B)$. Specialization is expressed by the rule $\bigwedge A\,\vdash AB$.
    
  \subsection{Uhh}
  
  \begin{align*}
    \mathrm{ReasonableEquality}\,E := \nameabst{\mathbb{T}}\ &\forall A, \forall B, \forall C,\\
      &\ (EAB \to (\mathbb{T}A \to \mathbb{T}B))\\
      &\wedge (EAB \to E(CA)(CB))\\
      &\wedge (EAB \to E(AC)(AB))\\
    \mathrm{Equal}\,A\,B := \nameabst{\mathbb{T}}\ &\exists E, \mathrm{ReasonableEquality}\,E \wedge EAB
  \end{align*}
  
  \subsection{Inference rules}
  
  IC is a \emph{proof system}; some formulas are \emph{true statements}, and we must describe \emph{inference rules} that can be used to derive one true statement from others. For example, this rule states that
  
  \begin{equation*}
    \tag{cut}
    ((A \to B) \wedge ((B \wedge C) \to D)) \to ((A \wedge C) \to D)\\
  \end{equation*}
  , like the familiar \emph{modus ponens}:
  
  \begin{equation*}
    \inferrule{A \to B\\\\A}{B}
  \end{equation*}
  
  The introspection principle asks that we define formulas to represent inference rules.
  In some sense, this is the \emph{only} thing we need to do – by the time we're done representing inference rules, the theory will be powerful enough to represent everything else.
  However, when we try to define the concept of inference rules from a completely blank slate, we quickly see that it's built out of several sub-concepts:
  \begin{itemize}
    \item \textbf{Unordered sets} of premises, which are other formulas.
    \item \textbf{Variables} (the A and B above), which express that you can inject arbitrary formulas into particular locations within the inference rule, and say that the rule exists for all such values.
    \item Finally, the fact that you can combine multiple inference rules to draw further conclusions.
  \end{itemize}
  
  As our way of defining how inference rules work \emph{inside} IC, we need to describe inference rules \emph{of} IC. This will look like a self-referential definition, where everything has to be defined before anything works. So we begin with intuitive definitions, then formalize them as we go.
    
  \subsection{Basics}
  
  $\top$ (read as "true") is the trivially true proposition.
  
  $A \wedge B$ (read as "A and B") is logical conjunction, with the usual meaning: $A \wedge B$ is true \emph{iff} $A$ and $B$ are both true.

  \newcommand{\objto}{\hookrightarrow}
  $\forall x, B$ (read as "for all x, B") is universal quantification. Here, $B$ is a formula that may contain usages of the name $x$. When we state a rule with unbound variables, they are implicitly quantified this way. (For example, a rule stated as $A \wedge B \vdash A$ is implicitly $\forall A, \forall B, A \wedge B \vdash A$.)
  
  Both $A \vdash B$ (read as "A proves B") and $A \objto B$ (read as "A implies B") are representations of inference rules or logical implication, saying that if A is true, then B is true. However, this needs more explanation.
  
  The usual propositional logic – \emph{unlike} IC – postulates \emph{modus ponens}, an inference rule which states:
  
  \begin{equation*}
    \tag{modus ponens}
    A \wedge (A \objto B) \vdash B\\
  \end{equation*}
  
  thus saying that if $(A \objto B)$ and $A$ are both \emph{true}, then $B$ is \emph{true}. IC only says that if $(A \objto B)$ is an \emph{inference rule of a system}, and $A$ is true in that system, then $B$ is true in that system. IC's basic rule of implication is the following, which keeps metatheory levels separate by having the \emph{same} number of levels of "$\objto$" in each premise and conclusion:
  
  \begin{equation*}
    \tag{chain rule}
    (A \objto B) \wedge (B \wedge C \objto D) \vdash (A \wedge C \objto D)\\
  \end{equation*}
  
  This can be read as modifying the rule $(B \wedge C \objto D)$, satisfying the need for $B$ by plugging the output of $(A \objto B)$ into it. It's a generalization of transitivity – by setting $C$ to $\top$, you get $(A \objto B) \wedge (B \objto D) \vdash (A \objto D)$. And if we set both $A$ and $C$ to $\top$, we get:
  
  \begin{equation*}
    (\top \objto A) \wedge (A \objto B) \vdash (\top \objto B)\\
  \end{equation*}
  
  which reveals the relation to \emph{modus ponens}: It's the same except with the trivial-looking "$\top \objto$" keeping everything at the same implication level.
  
  So, what's the relationship between $\vdash$ and $\objto$? They are the exact same concept, just at different metatheory levels. At first glance, $\vdash$ defines an inference rule of IC itself, while $\objto$ is an internal object that represents an inference rule. But it's more subtle than that: if $\vdash$-rules have $\objto$ on both sides, and $\objto$-rules are supposed to represent $\vdash$-rules, then $\objto$-rules must have some third relation (call it $\looparrowright$) on both sides. This continues infinitely, and the same rules must exist on every level.\footnote{Propositional logic avoids this by making all later relations the same as the second one, but accepting that they will have different rules than the first.}
  
  Ultimately, we want to write all the levels using the same symbol. We can't make that rigorous yet. For now, assume that each rule does exist on every level, and the symbols $\vdash$ and $\objto$ are \emph{variables} which can be filled in with any inference-level and its corresponding object-level.
  
  \subsection{Variables}
  
  I say \emph{variables}, but what we really need to define is \emph{functions}. Variables are just the convenient way of expressing them to the reader.\footnote{"But," you say, "doesn't be introspection principle ask that if you're discussing variables in the metatheory, you should define a representation of them within the theory?" Well, yes – but they don't need to be part of the \emph{core calculus}, which is the main job of \emph{this paper}. A full programming language, which I plan to build around this core calculus, would naturally be able to implement functions that convert text into token-trees and token-trees into formulas of IC, and others such inconveniences of interfacing between raw mathematical objects and the human reader.} A rule $A \wedge B \vdash A$ needs a function that takes two arguments, injects the first at the locations we labeled $A$, and injects the second at the locations we labeled $B$.
  
  
  \subsection{All rules of deduction}

  \begin{align*}
    \tag{and}
    (A \wedge B) &\vdash A\\
    \tag{symmetry}
    A \wedge B &\equiv B \wedge A\\
    \tag{associativity}
    (A \wedge B) \wedge C &\equiv A \wedge (B \wedge C)\\
    \tag{embed and}
    ((A \objto B) \wedge (A \objto C)) &\vdash (A \objto B \wedge C)\\
    \tag{embed And}
    (\forall X, A \objto B X) &\vdash (A \objto \bigwedge B)\\
    \tag{chain rule}
    (A \objto B) \wedge (B \wedge C \objto D) &\vdash (A \wedge C \objto D)\\
    A &\vdash \bigwedge \const A\\
    \bigwedge A &\vdash \bigwedge \fuse A\,B\\
    \tag{specialization}
    \bigwedge A &\vdash AB\\
    \const A\,B &\equiv A\\
    \fuse A\,B\,C &\equiv (AC)(BC)\\
    (A \equiv B) &\vdash (CA \equiv CB)\\
    (A \equiv B) &\vdash (AC \equiv BC)\\
  \end{align*}
  \newcommand{\deduction}{\operatorname{\mathrm{Deduction}}}
  Now, assemble all of these rules into one giant conjunction, which takes the relations $\vdash$ and $\objto$ as parameters. We will call this $\deduction$; the full definition goes like
  \begin{align*}
    \deduction& :=\\
    \nameabst{(\vdash)}&\ \nameabst{(\objto)}\\
    &\forall A, \forall B, (A \wedge B) \vdash A\\
    \wedge\ &\forall A, \forall B, A \wedge B \equiv B \wedge A\\
    \wedge\ &\dots\\
  \end{align*}
  
  And now we can define the \emph{single} symbol $(\to)$, which represents both inference rules of IC and internal representations of them.
  \\
  
  \textbf{Definition.} IC has $(\deduction\ (\to)\ (\to))$ as inference rules.
  
  \textbf{Definition.} IC has $(\deduction\ (\to)\ (\to))$ as a true statement, i.e. IC has $(\top \to \deduction\ (\to)\ (\to))$ as an inference rule.
  \\
  
  "But wait," you say, "if we can just plug in $(\to)$ as both inputs, why did we need to keep them separate?"
  
  Back to that in a minute. First, let's explore what statements count as true within the system so far.
  
  \subsection{Is \emph{modus ponens} true?}
  
  One may view a formal system is describing a \emph{set of true statements}. It does this by stating inference rules, which are \emph{constraints} on the set of true statements. In particular, inference rules are "positive constraints" – "if these premises are true, this conclusion must be true" – and never say that a statement must \emph{not} be true. Rather than having negative constraints, we just say that we're only interested in the set of statements that are \emph{forced} to be true by the inference rules – also known as the \emph{minimal relation obeying the inference rules}, or \emph{postulating induction on the structure of proofs}. And we hope that our system is \emph{consistent}, meaning that it does not force \emph{all} statements to be true.
  
  IC has an additional desire: Each true statement should represent a correct constraint on the set of true statements. And ideally, all correct constraints would also be true statements. But unfortunately, some constraints are self-incompatible: They are only correct if they are not also true statements (or if all statements are true). So we have to be slightly stricter about what statements can be "true".
  
  Let's consider the statement "Each true statement should represent a correct constraint on the set of true statements." By the introspection principle, we should be able to represent this statement in IC. Indeed, we can already represent the statement – and it is \emph{modus ponens!}
  
  "But," you say, "how does \emph{modus ponens} mean that?"
  
  Suppose you have a true statement $A \to B$. If we view this as a derived inference of IC, it is $\top \to (A \to B)$. If we also know that \emph{modus ponens}, also known as $(A \wedge (A \to B) \to B)$, is an inference of IC, then we can apply the chain rule – satisfying \emph{modus ponens}'s need for the premise $(A \to B)$ – and get $(A \wedge \top \to B)$, which reduces to $A \to B$, now on the level of an inference rather than a true statement. Thus, \emph{modus ponens} can be used to turn any true statement into an inference. This reflects our intent for the language; we must assert that it is a correct constraint.
  
  But what happens if we make it a true statement?
  
  \subsubsection{Russell's Paradox}
  
  First, we'll need some preliminaries.
  
  We define $\bot$ (read as "false") to equal $(\bigwedge \id)$, the proposition that all propositions are true. Clearly, this cannot be true in a consistent system.
  
  We would like to construct a statement $P$ such that $P \equiv P \to \bot$. To do this, we use Russell's Paradox, the "set of all sets which do not contain themselves". By convention, we represent \emph{sets} as predicates, and say that a set $S$ "contains" any $x$ for which $S x$ is true. So the "set of all sets which do not contain themselves" is:
  
  \begin{align*}
    R := \nameabst{x} x x \to \bot
  \end{align*}
  
  Now, our $P$ is simply $RR$:
  
  \begin{align*}
    P &:= RR\\
      &\equiv (\nameabst{x} x x \to \bot)R\\
      &\equiv RR \to \bot\\
      &\equiv P \to \bot\\
  \end{align*}
  
  Armed with this paradoxical statement, we can specialize \emph{modus ponens} with $A := P$ and $B := \bot$. The following statement is true, even if we do not assume \emph{modus ponens}:
  
  \begin{align*}
    modus\ ponens &\to (P \wedge (P \to \bot) \to \bot)\\
  \end{align*}
  
  But wait – $P \to \bot$ is just $P$, and $P \wedge P$ is just $P$, so this reduces:
  
  \begin{align*}
    modus\ ponens &\to (P \wedge (P \to \bot) \to \bot)\\
    modus\ ponens &\to (P \wedge P \to \bot)\\
    modus\ ponens &\to (P \to \bot)\\
    modus\ ponens &\to P\\
  \end{align*}
  
  If \emph{modus ponens} is both an inference rule and a true statement, we can now specialize the inference rule with $A := modus\ ponens$ and $B := P$, and thus infer that $P$ is true; and with one more use of the inference rule, we can infer that $\bot$ is true, and the system is inconsistent.
  
  Thus, \emph{modus ponens} is precisely one of our "self-incompatible constraints" – a constraint that is correct, but must not be a true statement! I choose to interpret this as \emph{modus ponens} being an "improper" rule of deduction, because of how it mixes truths with inferences, unlike all the rules in $\deduction$. Much of our remaining rules will be devoted to getting as close as possible to having \emph{modus ponens} be true, by adding a series of limited, safer versions of it.
  
  
    
  %(A \wedge (A \to B) \vdash B)
  
  \subsection{Induction on proof structure}
  
  Consider again the statement "Statements are only true if they're forced to be true by the inference rules." As with \emph{modus ponens}, we regard it to be true from the outside perspective. How shall we express this?
  
  We express it like this:
  
  \begin{align*}
    \tag{induction on proofs}\label{inp}
    \forall (\vdash), \forall A,\forall B,  (\deduction\ (\vdash)\ (\to)) \wedge (A \to B) \to (A \vdash B)\\
    \forall (\vdash), (\deduction\ (\vdash)\ (\to)) \to (A \to B) \to (A \vdash B)\\
    \forall (\models), (\deduction\ (\models)\ (\to)) \wedge (A \vdash B) \looparrowright (A \models B)\\
    (A \vdash B) \looparrowright \forall (\models), (\deduction\ (\models)\ (\to)) \looparrowright (A \models B)\\
  \end{align*}
  
  This deserves some explanation. Three different levels of inference are involved here (marked with numbers for the reader's benefit):
  
  \begin{align*}
    \forall (\vdash), \forall A,\forall B,  (\deduction\ (\vdash)\ (\implies{2})) \wedge (A \implies{1} B) \implies{0} (A \vdash B)\\
  \end{align*}
  
  We've already said that $(\deduction\ (\implies{1})\ (\implies{2}))$ is true. The idea now is to say that $\implies{1}$ is the \emph{minimal relation} obeying that constraint. We do this by saying that every other relation $\vdash$ obeying the same constraint is "at least as big" as $\implies{1}$, by saying that if $(A \implies{1} B)$, then $(A \vdash B)$ as well. ($(A \vdash B)$ may be true for \emph{more} things, but it can't be true for less.)
  
  This definition can also be used to write proofs by induction on the structure of proofs: If you describe an explicit $\vdash$, then the statements in $(\deduction\ (\vdash)\ (\implies{2}))$ are your induction cases.
  
  As with \emph{modus ponens}, we assert that this is a correct constraint on the true statements of IC. Again, this raises a question: Can \eqref{inp} itself be a true statement of IC?
  
  The answer, again, is no. Because $\implies{0}$ is equated with the other levels, this statement itself is a formula of the form $A \to B$, and it is not proved by $(\deduction\ (\to)\ (\to))$.
  
  
  \subsection{Uhh}
  
  We've now seen several constraints that cannot be true statements.
  
  Consider the statement "\emph{modus ponens} is a correct constraint (even though it may not be a true statement)". By the introspection principle, we should have a way to represent this statement within IC, and perhaps even have it be true.
  
  \newcommand{\icset}{\operatorname{\mathcal{IC}}}
  Let's have a symbol, $\icset$, which represents all inference rules of IC. (This will be self-containing, and that's okay.) Although we cannot soundly have $(\icset \to A) \to A$ as a truth, we are free to postulate that $\icset$ has the same \emph{set of true statements} as IC:
  
  \begin{align*}
    (\icset \to (\top \to A)) \equiv A\\
  \end{align*}
  
  We can now make statements about what inferences are allowed by $\icset$:
  
  \begin{align*}
    \icset \to modus\ ponens\\
  \end{align*}

  
  We'll have to refer to $(\deduction\ (\to)\ (\to))$ a lot. Let's define the shorthand $D_0 := (\deduction\ (\to)\ (\to))$.
  
  At this point, the \emph{truths} of IC are $D_0$ (by definition) and anything that can be deduced from it according to the inference rules, which are also $D_0$.
  
  We can also consider the statements $A$ where $(D_0 \to A)$ is true. This doesn't intrinsically include everything that's true in IC; if we added another rule ($\top \to Q$) to IC, it wouldn't give us ($\top \to (D_0 \to Q)$), because $Q$ cannot be deduced from just $D_0$, and ($\top \to Q$) does not add ($\top \to Q$) as a \emph{truth}, only $Q$. However, this category definitely contains $D_0$ and anything that can be deduced from it.
  
  Now, how about the statements $B$ where $(D_0 \to (D_0 \to B))$ is true?
  
  Clearly there's an infinite progression here. Viewing it from the outside, we believe that all of these categories contain the same statements. But we can't yet prove it within IC.
  
  The statement $\forall A, (D_0 \to A) \to A$
  
  \begin{align*}
    A\\
    D_0 \to A\\
    D_0 \to (D_0 \to A)\\
    D_0 \wedge (\top \to D_0) \to A\\
    D_0 \wedge (\top \to D_0) \to (\top \to A)\\
  \end{align*}


  The introspection principle asks us to be able to discuss \emph{functions}, not just notations for functions. The meaning of \emph{functions} is how they map inputs to outputs – their \emph{extensional} behavior.
  \begin{align*}
    \tag{extensionality}
    %(A \equiv B) \equiv \forall R, (\forall A,\forall B,) \to (A \equiv B)
    \forall R, (\varnothing \to R A B) \cup (\forall C,\forall D,(\forall X,R (C X) (D X)) \to R C D) \to (A \equiv B)\\
  \end{align*}

  \begin{align*}
    A pred1 &\to \bigwedge A \to AB\\
    A prop \wedge B prop &\to A \wedge B \to B \wedge A\\
    \tag{and}
    A prop \wedge B prop &\to (A \wedge B) \to A\\
    \tag{associativity}
    A prop \wedge B prop \wedge C prop &\to (A \wedge B) \wedge C \leftrightarrow A \wedge (B \wedge C)\\
    \tag{map and}
    A prop \wedge B prop \wedge C prop &\to ((A \to B) \wedge (A \to C)) \to (A \to (B \wedge C))\\
    \tag{map And}
    A pred1 &\to (\forall X, A \to B X) \to (A \to \bigwedge B)\\
    \tag{cut}
    ((A \to B) \cup ((B \wedge C) \to D)) &\to ((A \wedge C) \to (D))\\
    A &\to \bigwedge \const A\\
    \bigwedge A &\to \bigwedge \fuse A\,B\\
    \const A\,B &\leftrightarrow A
  \end{align*}
  \begin{align*}
    \bigwedge A &\to AB\\
    AB &\to \bigvee A\\
    A \wedge B &\to B \wedge A\\
    A \vee B &\to B \vee A\\
    \tag{and}
    (A \wedge B) &\to A\\
    \tag{or}
    A &\to (A \vee B)\\
    \tag{associativity}
    (A \wedge B) \wedge C &\leftrightarrow A \wedge (B \wedge C)\\
    \tag{associativity}
    (A \vee B) \vee C &\leftrightarrow A \vee (B \vee C)\\
    \tag{map and}
    ((A \to B) \wedge (A \to C)) &\to (A \to (B \wedge C))\\
    \tag{map And}
    (\forall X, A \to B X) &\to (A \to \bigwedge B)\\
    \tag{map or}
    ((A \vee B) \to C) &\to ((A \to C) \vee (B \to C))\\
    \tag{map Or}
    (\bigvee A \to B) &\to (\exists X, A X \to B)\\
    \tag{cut}
    ((A \to B \vee J) \wedge ((B \wedge C) \to D)) &\to ((A \wedge C) \to (D \vee J))\\
    \tag{cut}
    \neg ((\neg A \vee B \vee J) \wedge (\neg (B \wedge C) \vee D)) &\vee (\neg (A \wedge C) \vee (D \vee J))\\
    \tag{cut}
    (\neg (\neg A \vee B \vee J) \vee \neg (\neg (B \wedge C) \vee D)) &\vee (\neg (A \wedge C) \vee (D \vee J))\\
    \tag{cut}
    ((A \wedge \neg B \wedge \neg J) \vee ((B \wedge C) \wedge \neg D)) &\vee ((\neg A \vee \neg C) \vee (D \vee J))\\
    \tag{cut}
    (A \wedge \neg B \wedge \neg J) \vee (B \wedge C \wedge \neg D) &\vee \neg A \vee \neg C \vee D \vee J\\
    \tag{cut}
    \neg((\neg(A \to B) \to J) \to \neg (\neg (B \to \neg C) \to D)) &\to (\neg (A \to \neg C) \to (\neg D \to J))\\
    \bigvee \const A &\to A\\
    \bigvee \fuse A\, B &\to \bigvee A\\
    A &\to \bigwedge \const A\\
    \bigwedge A &\to \bigwedge \fuse A\,B\\
    \const A\,B &\leftrightarrow A
  \end{align*}
  \begin{align*}
    \const A\,B &\equiv A\\
    \fuse A\,B\,C &\equiv (AC)(BC)\\
    \tag{identity}
    A &\equiv A \cup \varnothing\\
    \tag{symmetry}
    A \cup B &\equiv B \cup A\\
    \tag{associativity}
    (A \cup B) \cup C &\equiv A \cup (B \cup C)\\
    \tag{specialization}
    \bigcup A &\to A B\\
    \tag{specialization}
    \bigcup \const A &\equiv A\\
    \tag{specialization}
    \bigcup A &\to \bigcup \fuse A B\\
    \tag{specialization}
    \bigcup A &\equiv \forall B, \bigcup \fuse A B\\
    \tag{uhh}
    (A \to B) &\equiv (A \to (A \cup B))\\
    \tag{uhh}
    (A \cup B) &\to A\\
    \tag{uhh}
    (A \equiv B) &\equiv ((A \to B) \cup (B \to A))\\
    \tag{uhh}
    (A \equiv B) &\to (A \to B)\\
    \tag{indistinguishability}
    (A \equiv B) &\to (CA \equiv CB)\\
    \tag{extensionality}
    (A \equiv B) &\to (AC \equiv BC)\\
    \tag{unioning}
    ((A \to B) \cup (A \to C)) &\to (A \to (B \cup C))\\
    \tag{univ. unioning}
    ((\forall X, A \to B X) &\to (A \to \bigcup B)\\
    \tag{chaining}
    ((A \to B) \cup ((B \cup C) \to D)) &\to ((A \cup C) \to D)\\
  \end{align*}

  
  
  \section{Fundamentals}\label{fundamentals}

  \subsection{Syntax}
  \begin{align*}
     Atom :=&\ \mathrm{level\_zero} \mid \mathrm{level\_successor} \mid \mathrm{implies} \mid \mathrm{equals} \mid \mathrm{const} \mid \mathrm{fuse} \mid \mathrm{induction\_on\_proofs}\\
     F :=&\ Atom \mid (F F)
  \end{align*}

  \subsection{Notations}

  % give examples first????
  % pronunciations of the symbols
  % AB is application
  % abstractions can be viewed as forall, lambda, function, predicates
  % substution better notation?
  % implication not subscript?
  % clean up explanation of metavariable levels WRT named abstractions
%  \[ A\ B\ C \dots Y\ Z := ((\dots ((A B) C) \dots Y) Z) \]

  \newcommand{\ic}[1]{#1}
%  \newcommand{\abst}[2]{{#1} \langle \overline{\underline{ #2}}]}
  
  
  \newcommand{\equals}{\equiv}

  \begin{align*}
    (A \equals B) &:= ((\mathrm{equals}\ A) B)\\
    \id &:= \fuse \const \const\\
  \end{align*}

  Finally, we define the \emph{named form} of abstractions, $(\nameabst{A}B),\ \operatorname{right-associative}$, where $A$ is a variable-name, and $B$ may contain instances of that name. (Perhaps we should say \emph{metavariable-name} rather than \emph{variable-name}, as these names only exist for this notation, and are not part of the formal language of IC).
  
  This is defined as in the SKI combinator calculus.

  \subsection{Internal Axioms}

  We now define \emph{internal} representations of the above axioms.

  From the internal perspective of the calculus, the above axioms provide truth for any \emph{specific} values of the variables $A, B\dots$, but cannot prove abstractions over those variables.
  So, for each axiom, we also create an internal form, by representing the metavariables using abstractions, and the premises using implication.
  For example, (modus ponens) is represented as:

  \begin{equation*}
    \mathrm{``modus\ ponens"} := \nameabst{A} \nameabst{B} \nameabst{n} (A \implies{n} B) \to A \to B
  \end{equation*}
  
  The above is merely a notation, saying that the right-hand side \emph{is} the statement of "modus ponens". Rather than postulate each of these generalized axioms individually, we make the following comprehensive rule:


  (We would like the definition of proof induction to be a notation like the other axioms, but that would make it a self-containing formula. IC can actually permit self containing formulas, but we understand that the reader may be skeptical of a self containing axiom! Thus, we choose to represent it as an atom with a definitional-equality.)
  
  Observe that this rule immediately proves any of the individual generalized axioms. For example, "modus ponens" is proved by specializing this rule with $P := \mathrm{``modus\ ponens"}$, because the premise $R\mathrm{``modus\ ponens"}$ proves the conclusion $RP$.
  
  But the more interesting feature of this rule is the other direction of implication: If $P$, then $P$ is reachable by induction from the listed axioms. This essentially says that our list of axioms is \emph{exhaustive} – there are no other axioms – and the rule makes that fact visible to internal logic.
  
With such a powerful rule, one should immediately worry if it proves a contradiction. After all, the generalized axioms are not strictly the same objects as the external axioms, and the axiom \eqref{indd} is not in the list at all. However, any statement you can prove using an external axiom is a specialization of the generalized form, and thus reachable by induction from the generalized axioms. And because definitionally-equal formulas are indistinguishable to the internal logic, \eqref{indd} is just a specialization of \eqref{eq_refl}.

  \section{Examples}\label{structure}

  We can define the conventional logical connectives in terms of the above rules:

  \setlength{\jot}{0.4em}
  \begin{align*}
    \tag{false/absurdity}
    \bot &:= (\nameabst{P}{P})\\
    \tag{negation}
    \neg_{n} P &:= ({P \implies{n} \bot})\\
    \mathrm{not} &:= (\nameabst{P} \neg P)\\
%    \tag{conjunction}
%    P \land Q &:= (\nameabst{R} (P \implies{\wfz} Q \implies{\wfz} R) \implies{\wfz} R)\\
%    \mathrm{and} &:= (\nameabst{P}\nameabst{Q} P \land Q)\\
%    \tag{disjunction}
%    P \lor Q &:= (\nameabst{R} (P \implies R) \implies (Q \implies R) \implies R)\\
%    \mathrm{or} &:= (\nameabst{P}\nameabst{Q} P \lor Q)\\
%    \tag{existential quantification}
%    \exists x, P(x) &:= (\nameabst{R} (\nameabst{x} P(x) \implies R) \implies R)\\
  \end{align*}

%  And we can prove some tautologies:
%  \begin{align*}
%  (P \land Q)
%  \end{align*}
\iffalse
  \subsection{Well-founded sets}

  Some discussion of well-founded sets, and how they avoid the Burali-Forti paradox.

  By convention, a \emph{set} is a predicate on formulas, where if $PF$ is true, then $F \in P$. One might expect this to cause paradoxes; this section will illustrate how it does not.

  We define \emph{well-founded sets} with a surprisingly simple rule: A well-founded set is a set for which all \emph{raisable predicates} are true. And a \emph{raisable predicate} $R$ is one where, if the $R$ is true for all members of a set, then $R$ is also true for the set itself:

  \ic{
  Raisable := R => (Q => (x => (Q x) -> (R x)) -> (R Q)).
  WellFounded := S => R => (Raisable R) -> (R S).
  }

  The first well-founded set is the empty set, which we can prove is well-founded:
  
%  \ic{
%  EmptySet := n => (x => x).
%
%  (WellFounded EmptySet) = (R => (Q => (m => (Q m) -> (R m)) -> (R Q)) -> R(n => (x => x))).
%
%  (Some axiom) proves ((R m) -> (R m))
%  (Generalization) proves (m => (R m) -> (R m))
%  (Specialization) proves (Q => (m => (Q m) -> (R m)) -> (R Q))
%                       -> (Q => (m => (Q m) -> (R m)) -> (R Q))EmptySet
%  = (m => (EmptySet m) -> (R m)) -> (R EmptySet)
%  = (m => (x => x) -> (R m)) -> (R EmptySet)
%  (Predicate unfolding) proves (Proven_a (Q => (m => (Q m) -> (R m)) -> (R Q))EmptySet) -> (m => (EmptySet m) -> (R m)) -> (R EmptySet)
%
%
%  }

  \section{Consistency}\label{consistency}
  
  We regard a proposition $P$ as "proven" if it is \emph{eventually true} within IC, i.e.
  
  \newcommand{\eventually}{\operatorname{\mathrm{Eventually}}}
  
  \[ \eventually P := \exists n, \clocksub{+n} P \]
  
  This is a meta-theoretical definition; if we use our internal definition of $\exists$, which contains 2 nested abstractions, then you can never prove $(\eventually P)$, only $\clocksub{+2}(\eventually P)$.
  
  That said, since every axiom of IC can be applied within clocks, we could view the inside of $\clocksub{+n}$ as being another instance of the calculus – call it IC$_{+n}$ – which adds the axiom "if $\clocksub{+n} P$ is true in IC, then $P$ is true in IC$_{+n}$". If we move to IC$_{+2}$, we \emph{can} be comfortable with an internal definition of $\eventually$. Thus, IC can function as its own metatheory, just with a clock offset.
  
  By the axiom \eqref{lne}, anything that's eventually true is also eternally true, in the sense that you can "wait" for an IC$_{+n}$ where P is as old as you want:
  
%  \[ \nameabst{n} \wellfounded n \implies \eventually P \implies \eventually \clocksub{-n} P \]
  
  One might ask: What if you can't prove $(\eventually P)$, but only $\eventually (\eventually P)$? One might hope that $\eventually (\eventually P)$ implies $(\eventually P)$, but it does not – it only implies $\clocksub{+constant}(\eventually P)$. But could we also regard that as "proving $P$"? Unfortunately, if we did, we would then want to extend it to $\eventually (\eventually (\eventually P))$, and so forth. For the sake of having a single definition, the answer must be \emph{no}. But the purpose of including the entire well-founded hierarchy is that you don't \emph{need} to.
  
  Intuitively, the only way to prove $(\eventually P)$ is to exhibit a \emph{specific} $n$ where you can prove $\clocksub{+n} P$. Assuming this is true, you can simply take that value of $n$ Unfortunately, you can't prove this within IC, because the internal logic of IC doesn't know that there aren't extra axioms that could make $\eventually P$ true for other reasons. However, what you \emph{can} do is to define a predicate
  
  \newcommand{\provable}{\operatorname{\mathrm{Provable}}}
  
  \[\provable P := \dots\]
  
  which is true for anything that can be proved from the actual axioms of IC. (You define this as an induction predicate, with the axioms of IC as cases; we omit the definition because it is long.)
  
  A digression on the properties of $\provable$: Meta-theoretically, we don't think you can prove anything that's not $\eventually \provable$. But this is impossible to prove inside IC, for the same reasons as above. In fact, the proposition $(P \implies \eventually (\provable P))$ is \emph{false} within IC, because it is self-contradictory. (If true, it would be a statement that was true but not provable from the axioms!)
  
%  We also cannot prove $(\provable P) \implies P$, even though we believe that all provable statements are true; but fortunately, $(\provable P) \implies (\eventually P)$ is a trivial corollary.
  
  (All of the following is conjecture, but I expect it to be true, at least once I have cleaned up any definitional mistakes:)
  
  With the provability predicate, we can prove:
  
%  \[ \nameabst{P} \eventually (\provable (\eventually P)) \implies \eventually  P \]
  
  This theorem has an immediate corollary that is very significant:
  
%  \[ \nameabst{P} \eventually (\provable (\eventually \bot)) \implies \eventually \bot \]
\fi

  %\printbibliography
\end{document}
